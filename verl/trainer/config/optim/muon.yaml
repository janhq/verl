# Muon optimizer configuration for FSDP training.
#
# Muon (MomentUm Orthogonalized by Newton-Schulz) is designed for 2D matrix parameters
# in neural networks. It should be combined with AdamW for non-matrix parameters.
#
# Reference: https://github.com/KellerJordan/Muon

# Target class for this configuration
_target_: verl.workers.config.FSDPOptimizerConfig

# Optimizer class name
# Options: "MuonWithAdamW", "Muon"
optimizer: MuonWithAdamW

# Module path to import optimizer from
# For Muon, use verl.optim.muon
optimizer_impl: verl.optim.muon

# Learning rate for Muon (2D parameters like W_q, W_k, W_v, W_o, mlp layers)
# Typical value: 0.02
lr: 0.02

# Learning rate for embeddings/non-matrix parameters
# If null, defaults to lr * 10 (i.e., 0.2)
lr_embedding: null

# LR warmup steps ratio
lr_warmup_steps_ratio: 0.0

# Total training steps
total_training_steps: -1

# Weight decay for Muon (set to 0 when using MuonWithAdamW)
# Weight decay is handled separately for AdamW fallback
weight_decay: 0.0

# LR warmup steps
lr_warmup_steps: -1

# Momentum factor for Muon
# Typical value: 0.95
momentum: 0.95

# Use Nesterov momentum for Muon
# Typical value: True
nesterov: true

# Number of Newton-Schulz iterations for orthogonalization
# Typical value: 5
ns_steps: 5

# Clip gradient
clip_grad: 1.0

# Learning rate for AdamW fallback (non-matrix parameters like embeddings, biases)
# Typical value: 3e-4
adamw_lr: 3e-4

# AdamW betas
adamw_betas: [0.9, 0.95]

# AdamW weight decay
# Typical value: 0.1
adamw_weight_decay: 0.1

# Minimum LR ratio for cosine schedule
min_lr_ratio: 0.0

# Number of cosine cycles in LR schedule
num_cycles: 0.5

# LR scheduler type: "constant" or "cosine"
lr_scheduler_type: constant

# Additional optimizer-specific keyword arguments
override_optimizer_config: null